{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3945a9da-fce2-42b2-ae24-adaecdb5eae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 📊 Creating Fake DataFrames for Explanation  \n",
    "\n",
    "After the training process, we will have three key datasets:  \n",
    "\n",
    "- **Training DataFrame**: Used to train the model.  \n",
    "- **Validation DataFrame**: Used to tune the model's hyperparameters.  \n",
    "- **Testing DataFrame**: Used to evaluate the final model performance.  \n",
    "\n",
    "For the sake of this example, let's assume this process is already completed. Below, we generate sample DataFrames to demonstrate How to use The Ml Eval Lib. 📝🔍  \n",
    "\n",
    "⚠ Note:\n",
    "\n",
    "The validation_dataframes is required for the library to operate.\n",
    "train_dataframes and test_dataframes are optional but useful for more comprehensive model evaluations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53794bf1-8769-4ab7-8420-13b6b431e459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Generate synthetic data for demonstration\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Simulate some categorical and continuous features for segmentation\n",
    "np.random.seed(42)\n",
    "age = np.random.randint(20, 60, size=1000)\n",
    "income = np.random.randint(30000, 100000, size=1000)\n",
    "gender = np.random.choice(['M', 'F'], size=1000)\n",
    "region = np.random.choice(['Urban', 'Suburban', 'Rural'], size=1000)\n",
    "education = np.random.choice(['High School', \"Bachelor's\", \"Master's\", 'PhD'], size=1000)\n",
    "marital_status = np.random.choice(['Single', 'Married', 'Divorced'], size=1000)\n",
    "\n",
    "# Add a timestamp column with random dates within a range\n",
    "start_date = pd.Timestamp('2023-01-01')\n",
    "end_date = pd.Timestamp('2023-12-31')\n",
    "timestamps = pd.to_datetime(np.random.randint(start_date.value // 10**9, end_date.value // 10**9, size=1000), unit='s')\n",
    "\n",
    "# Add those features to the data\n",
    "features_df = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Income': income,\n",
    "    'Gender': gender,\n",
    "    'Region': region,\n",
    "    'Education Level': education,\n",
    "    'Marital Status': marital_status,\n",
    "    'Timestamp': timestamps\n",
    "})\n",
    "\n",
    "# Combine synthetic features with the classification problem for train, val, test sets\n",
    "train_features = features_df.iloc[:len(X_train)]\n",
    "val_features = features_df.iloc[len(X_train):len(X_train)+len(X_val)]\n",
    "test_features = features_df.iloc[len(X_train)+len(X_val):]\n",
    "\n",
    "# Train four models\n",
    "model1 = LogisticRegression(max_iter=1000)\n",
    "model2 = RandomForestClassifier()\n",
    "model3 = SVC(probability=True)\n",
    "model4 = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Fit models\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "# Function to create predictions dataframe\n",
    "def create_predictions_df(model, X, y, features, dataset_type):\n",
    "    return pd.DataFrame({\n",
    "        \"ID\": np.arange(len(X)),\n",
    "        \"Age\": features['Age'].values,\n",
    "        \"Income\": features['Income'].values,\n",
    "        \"Gender\": features['Gender'].values,\n",
    "        \"Region\": features['Region'].values,\n",
    "        \"Education Level\": features['Education Level'].values,\n",
    "        \"Marital Status\": features['Marital Status'].values,\n",
    "        \"Timestamp\": features['Timestamp'].values,\n",
    "        \"Target (True Label)\": y,\n",
    "        \"Prediction\": model.predict(X),\n",
    "        \"Prediction_Probabilities y(1)\": model.predict_proba(X)[:, 1].round(2)\n",
    "    })\n",
    "\n",
    "# Generate predictions for each model\n",
    "logistic_train = create_predictions_df(model1, X_train, y_train, train_features, \"train\")\n",
    "logistic_val = create_predictions_df(model1, X_val, y_val, val_features, \"val\")\n",
    "logistic_test = create_predictions_df(model1, X_test, y_test, test_features, \"test\")\n",
    "\n",
    "rf_train = create_predictions_df(model2, X_train, y_train, train_features, \"train\")\n",
    "rf_val = create_predictions_df(model2, X_val, y_val, val_features, \"val\")\n",
    "rf_test = create_predictions_df(model2, X_test, y_test, test_features, \"test\")\n",
    "\n",
    "svm_train = create_predictions_df(model3, X_train, y_train, train_features, \"train\")\n",
    "svm_val = create_predictions_df(model3, X_val, y_val, val_features, \"val\")\n",
    "svm_test = create_predictions_df(model3, X_test, y_test, test_features, \"test\")\n",
    "\n",
    "xgb_train = create_predictions_df(model4, X_train, y_train, train_features, \"train\")\n",
    "xgb_val = create_predictions_df(model4, X_val, y_val, val_features, \"val\")\n",
    "xgb_test = create_predictions_df(model4, X_test, y_test, test_features, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836cede8-3933-4bb8-9a45-ddb38528ce0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Display  how  the Dataframe  looks like "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17102cd-364f-48e0-844c-9aa8f97e3625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ⚙️ Parameters  of Ml Evaluation FrameWork \n",
    "\n",
    "### 1. 🏷️ `model_names` (*List of Strings*)  \n",
    "\n",
    "- **📌 Description**: A list containing the names of machine learning models to be evaluated.  \n",
    "- **📝 Example**: `[\"logistic_regression\", \"random_forest\"]`  \n",
    "\n",
    "### 2. 📊 `train_dataframes` (*List of Pandas DataFrames, optional*)  \n",
    "\n",
    "- **📌 Description**: A list of training datasets corresponding to each model. Optional parameter.  \n",
    "- **📝 Example**: `[training_dataframe_model_1, training_dataframe_model_2]`  \n",
    "\n",
    "### 3. 🧪 `validation_dataframes` (*List of Pandas DataFrames, required*)  \n",
    "\n",
    "- **📌 Description**: A list of validation datasets that will be used to assess model performance. **Mandatory** for the library to function.  \n",
    "- **📝 Example**: `[validation_dataframe_model_1, validation_dataframe_model_2]`  \n",
    "\n",
    "### 4. 🎯 `test_dataframes` (*List of Pandas DataFrames, optional*)  \n",
    "\n",
    "- **📌 Description**: A list of test datasets used for final evaluation after model training.  \n",
    "- **📝 Example**: `[test_dataframe_model_1, test_dataframe_model_2]`  \n",
    "\n",
    "### ⚠️ **Important Notes**  \n",
    "\n",
    "- ✅ `validation_dataframes` is **required** for the library to operate.  \n",
    "- ⚡ `train_dataframes` and `test_dataframes` are **optional** but useful for more comprehensive model evaluations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc800649-35ff-49a2-b84f-bbdda477da79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🔹 Step 1: Define Model Names and Load Data  \n",
    "\n",
    "To use this library, you first need to 🏗️ define the models and 📥 load the relevant datasets.  \n",
    "Store 🏋️ training, 🧪 validation, and 🎯 test data in lists. \n",
    "\n",
    "### ⚠ **Note**:\n",
    "\n",
    "- The `validation_dataframes` is required for the library to operate.\n",
    "- `train_dataframes` and `test_dataframes` are optional but useful for more comprehensive model evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0363939-0f05-4aed-a6ac-c08cd8410044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# classification parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80be03cb-be84-41c7-82b0-3f2255a0b0ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 🏷️ List of Model Names  \n",
    "model_names = [\"Old Model (logistic regression)\", \"Challenger Model Random  Forest \", \"Challenger Model SVM \", \"Challenger Model XGBoost\"]  \n",
    "\n",
    "# 📊 Training DataFrames for Each Model  \n",
    "train_dataframes = [logistic_train, rf_train, svm_train, xgb_train]  \n",
    "\n",
    "# 🧪 Validation DataFrames for Model Performance Assessment  \n",
    "validation_dataframes = [logistic_val, rf_val, svm_val, xgb_val]  \n",
    "\n",
    "# 🎯 Test DataFrames for Final Evaluation  \n",
    "test_dataframes = [logistic_test, rf_test, svm_test, xgb_test]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee55e50e-472c-49d2-b64a-d977ba42fc64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🛠️ Step 2  : Modify Configuration Files (if needed)  \n",
    "\n",
    "If custom configurations are required, you can adjust the ⚙️ settings by editing the YAML file:  \n",
    "\n",
    "📂 **model_evaluation/settings/model_evaluation_settings.yaml**  \n",
    "\n",
    "```yaml\n",
    "output_directory: \"../eda_lib/model_evaluation/output/\"\n",
    "# 📂 The directory where all output files (e.g., logs, evaluation metrics) generated during model evaluation will be saved.\n",
    "# ⚠️ Ensure this path exists beforehand and has the necessary write permissions to avoid runtime issues.\n",
    "\n",
    "parameters:\n",
    "\n",
    "    ai_feedback: False\n",
    "    # 🤖 A boolean flag to indicate whether AI-generated feedback will be utilized during the evaluation process.\n",
    "    # ✅ Set to `True` to include AI feedback integration, or leave as `False` to disable it.\n",
    "    # ⚠️ Note: The AI feedback feature is not fully tuned yet; keeping this set to `False` is recommended for now.\n",
    "\n",
    "    task_type: \"classification\"\n",
    "    # 🎯 Defines the type of task being evaluated. The selected task type impacts the metrics and logic applied during evaluation.\n",
    "    # 🔹 Possible options:\n",
    "    #   - \"classification\" 🏷️: For tasks such as binary or multi-class classification.\n",
    "    #   - \"regression\" 📈: For tasks that predict continuous values.\n",
    "\n",
    "time_based_analysis_parameters:\n",
    "\n",
    "    activate_analysis: True\n",
    "    # ⏳ Boolean flag to activate or deactivate time-based analysis.\n",
    "    # ✅ Set to `True` to enable evaluations that consider temporal trends in the data.\n",
    "\n",
    "    timestamp_column: \"Timestamp\"\n",
    "    # 📅 The name of the column containing timestamp information in the dataset.\n",
    "    # ⏰ This column will be used to perform time-based segmentations.\n",
    "\n",
    "    number_of_months: 4\n",
    "    # 📆 Specifies the time interval (in months) to use for the time-based analysis.\n",
    "    # 🔹 Examples:\n",
    "    #   - Use `3` for quarterly analysis. 📊\n",
    "    #   - Use `12` for annual analysis. 📅\n",
    "\n",
    "non_time_based_analysis_parameters:\n",
    "\n",
    "    activate_analysis: True\n",
    "    # 🏷️ Boolean flag to enable or disable non-time-based segmentation analysis.\n",
    "    # ✅ Set to `True` to include evaluations based on subcategories defined in the dataset.\n",
    "\n",
    "    subcategory_threshold: 10\n",
    "    # 🔢 The maximum number of subcategories to process during non-time-based segmentation analysis.\n",
    "    # ⚠️ If the number of subcategories exceeds this threshold, optimizations or exclusions may be applied.\n",
    "\n",
    "    segmentation_column: \"Gender\"  \n",
    "    # 🏷️ The column name used for segmentation in non-time-based analysis.\n",
    "    # 🔹 Examples:\n",
    "    #   - Use \"Gender\" 🚻 for gender-based segmentation.\n",
    "    #   - Use other categorical columns as needed for task-specific segmentation.\n",
    "\n",
    "display_settings:\n",
    "\n",
    "    side_bar_title: \"📊 Master ML Evaluation Framework\"\n",
    "    # 🖥️ The title displayed in the sidebar of the evaluation interface.\n",
    "    # 🎨 Customize this title to reflect the organization or framework's branding.\n",
    "\n",
    "    side_bar_logo: \"🌐 https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTBv8lPPqw_8NVRq01U8UhNguSO-Z6gdTlJjA&s\"\n",
    "    # 🖼️ The URL of the logo to be displayed in the sidebar of the evaluation interface.\n",
    "    # ⚠️ Ensure the URL is accessible and points to a valid image file to avoid display errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c768a7d-1a93-4286-b4aa-a0d3864f4a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step 3: Running the Model Evaluation 🧑‍💻📊  \n",
    "\n",
    "\n",
    "##### 🧠 Call the `MlEvaluationApp` Class  \n",
    "\n",
    "To perform model evaluation, we will call the **`MlEvaluationApp`** class from the following module:  \n",
    "`model_evaluation.model_evaluation_lib.model_evaluation_app_runner`.\n",
    "\n",
    "Once imported, the `run_app` function will execute the analysis.\n",
    "\n",
    "The output results will be saved in the directory:  \n",
    "`model_evaluation/output`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6597c5ab-7bc7-4840-bc3e-b3fd75874458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from model_evaluation.model_evaluation_lib.model_evaluation_app_runner import MlEvaluationApp\n",
    "\n",
    "\n",
    "app = MlEvaluationApp(\n",
    "    models=model_names,\n",
    "    training_dataframes_list=train_dataframes,\n",
    "    validation_dataframes_list=validation_dataframes,\n",
    "    test_dataframes_list=test_dataframes,\n",
    "    user_settings_path='model_evaluation/settings/model_evaluation_settings.yaml',\n",
    "    project_root_name=\"eda_lib\",\n",
    "    base_path= \"/Workspace/Repos/mohamed-naceur.mahmoud@telefonica.de\",\n",
    "    project_folder_name=\"classification\",\n",
    "    output_path=\"model_evaluation/output_folder\" ,\n",
    "    project_name=\"classification use case\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37bf63c-6b19-4e61-ac56-ddf44a6260af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "app.run_app()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "How To Use Model Eval Framework Classification Use Case",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
